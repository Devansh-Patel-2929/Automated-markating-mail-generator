{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7927404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Disables TensorFlow logging\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fe1bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Cell 1 (data loading and preprocessing)\n",
    "import pandas as pd\n",
    "\n",
    "# Load only first 5k entries\n",
    "df = pd.read_csv('emails.csv', names=['file', 'content'], header=None, nrows=1000)  # <-- LIMIT HERE\n",
    "\n",
    "def parse_email(content):\n",
    "    headers = {}\n",
    "    body = []\n",
    "    in_headers = True\n",
    "    for line in content.split('\\n'):\n",
    "        if in_headers:\n",
    "            if line.strip() == '':\n",
    "                in_headers = False\n",
    "            else:\n",
    "                if ':' in line:\n",
    "                    key, value = line.split(':', 1)\n",
    "                    headers[key.strip()] = value.strip()\n",
    "        else:\n",
    "            body.append(line)\n",
    "    return headers, '\\n'.join(body)\n",
    "\n",
    "df[['headers', 'body']] = df['content'].apply(lambda x: pd.Series(parse_email(x)))\n",
    "df['To'] = df['headers'].apply(lambda x: x.get('To', ''))\n",
    "df['Subject'] = df['headers'].apply(lambda x: x.get('Subject', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aef18307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine To, Subject, and body into a single training string\n",
    "df['training_text'] = 'To: ' + df['To'] + '\\nSubject: ' + df['Subject'] + '\\n\\n' + df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1399bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Cell 3 (dataset preparation)\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Explicitly create labels for language modeling\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': encoding['input_ids'].flatten().clone()  # THIS IS CRUCIAL\n",
    "        }\n",
    "\n",
    "# Load tokenizer with GPU support\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataset = EmailDataset(df['training_text'].tolist(), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "140c5502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: True\n",
      "GPU device: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 19:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.769500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.931600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.851300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./enron-email-generator\\\\tokenizer_config.json',\n",
       " './enron-email-generator\\\\special_tokens_map.json',\n",
       " './enron-email-generator\\\\vocab.json',\n",
       " './enron-email-generator\\\\merges.txt',\n",
       " './enron-email-generator\\\\added_tokens.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Updated Cell 4 (model training)\n",
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Initialize model with explicit device map\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "\n",
    "# Optimized training args for GPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,  # Better GPU memory utilization\n",
    "    fp16=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    report_to=\"none\",  # Disables external services\n",
    "    save_strategy=\"no\",  # Reduces file I/O\n",
    "    disable_tqdm=False  # Ensures progress bars work\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained('./enron-email-generator')\n",
    "tokenizer.save_pretrained('./enron-email-generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a49ad388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------- Forwarded by Phillip K Allen/HOU/ECT on 09/29/2000 \n",
      "10:11 AM ---------------------------\n",
      "\n",
      "\n",
      "Hunter S Shively\n",
      "09/29/2000 02:39 PM\n",
      "To: Phillip K Allen/HOU/ECT@ECT\n",
      "cc:    \n",
      "Subject: Project Update\n",
      "\n",
      "Hunter,\n",
      "\n",
      "Here is a project update for the west desk.  \n",
      "\n",
      "I will try and get back to you as soon as possible.\n",
      "\n",
      "Phillip\n"
     ]
    }
   ],
   "source": [
    "def generate_email(to, subject, max_length=600):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    prompt = f\"To: {to}\\nSubject: {subject}\\n\\nBody:\\n\"  # Explicit body section\n",
    "    \n",
    "    # Configure tokenizer for generation\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors='pt', return_attention_mask=True).to(device)\n",
    "    \n",
    "    # Generation parameters\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.9,  # More creative\n",
    "        top_p=0.92,       # Nucleus sampling\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.encode(\"<|endoftext|>\")[0],\n",
    "        num_beams=3,      # Beam search for better coherence\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Decode and clean output\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract content after Body: and before end token\n",
    "    try:\n",
    "        body_start = full_text.index(\"Body:\\n\") + 6\n",
    "        body_end = full_text.index(\"\\n\\n<|endoftext|>\")\n",
    "        return full_text[:body_start] + full_text[body_start:body_end]\n",
    "    except ValueError:\n",
    "        return full_text.split(\"Body:\\n\")[-1].split(\"<|endoftext|>\")[0]\n",
    "\n",
    "# Example usage\n",
    "print(generate_email(\"client@company.com\", \"Project Update\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
